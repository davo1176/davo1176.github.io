<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Web Scraper Application">
    <title>Web Scraper Application</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>Web Scraper Application</h1>
        <nav>
            <a href="../index.html">Back to Portfolio</a>
          </nav>
    </header>

    <section id="project-details">
        <h2>Project Overview</h2>
        <p>
            This project provides automated scripts to collect real-time snow report data from two different websites:
            <strong>Colorado Ski</strong> and <strong>OnTheSnow</strong>. By leveraging Python’s Selenium library and BeautifulSoup,
            the scraper captures details such as snowfall, base depth, open runs, and lifts open. The collected data are consolidated
            into CSV files for further analysis or integration into dashboards.
        </p>

        <h2>Objectives</h2>
        <ul>
            <li>Automate retrieval of up-to-date ski resort snow statistics.</li>
            <li>Combine data from multiple sources into a single dataset.</li>
            <li>Support historical trend analysis for snowfall and ski resort conditions.</li>
        </ul>

        <h2>Key Results</h2>
        <ul>
            <li>Scripts successfully fetch snow stats and resort availability data from two separate sites.</li>
            <li>Data is appended daily into CSV files, building a historical dataset.</li>
            <li>Resilient to dynamic page loads using Selenium’s <code>WebDriverWait</code> for accurate scraping.</li>
            <li>Includes custom logging to help track runs, errors, and data integrity.</li>
        </ul>

        <h2>Methodology</h2>
        <ul>
            <li>
                <strong>Selenium + ChromeDriver:</strong> Automates browser actions and waits for dynamic elements (e.g., 24-hour snowfall).
            </li>
            <li>
                <strong>BeautifulSoup:</strong> Parses the HTML after page load to extract text and numerical values.
            </li>
            <li>
                <strong>Data Aggregation:</strong> Each run appends new data to an existing CSV to build a time-series dataset.
            </li>
            <li>
                <strong>Error Handling & Logging:</strong> Detailed logs for error detection and script debugging.
            </li>
        </ul>

        <h2>Skills Demonstrated</h2>
        <ul>
            <li><strong>Web Scraping:</strong> Handling dynamic sites using Selenium waits and parsing with BeautifulSoup.</li>
            <li><strong>Data Wrangling:</strong> Combining multiple data sources into CSV, cleaning irregular strings, and normalizing data fields.</li>
            <li><strong>Python Scripting:</strong> Managing environment paths, operating system checks, and structured logging.</li>
            <li><strong>Version Control:</strong> Storing and updating code via Git and GitHub.</li>
        </ul>

        <h2>Technologies Used</h2>
        <ul>
            <li>Python (Selenium, BeautifulSoup, pandas, re, logging)</li>
            <li>ChromeDriver</li>
            <li>CSV for data storage</li>
        </ul>

        <h2>Scripts</h2>
        <p>
            Below is an abbreviated version of the two scripts used to scrape data from
            <strong><em>coloradoski.com</em></strong> and <strong><em>onthesnow.com</em></strong>.  
            (You can place them in separate Python files, e.g., <code>colorado_ski_scraper.py</code> and <code>onthesnow_scraper.py</code>.)
        </p>
        
        <h3>Colorado Ski Scraper</h3>
<pre><code>
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

driver_path = "C:/path/to/chromedriver.exe"
csv_file_path = "C:/path/to/snow_report_data_coloradoski.csv"

# Set up and launch Chrome
service = Service(driver_path)
driver = webdriver.Chrome(service=service)
driver.get("https://www.coloradoski.com/snow-report/")

# Wait until mid-mountain depth loads
WebDriverWait(driver, 30).until(
    lambda d: any(
        mid.text.strip() != '0"'
        for mid in d.find_elements(By.CLASS_NAME, "answer.mid-mtn")
    )
)

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
driver.quit()

snow_data = []
resorts = soup.find_all("div", class_="inner")

for resort in resorts:
    name_tag = resort.find("h3", class_="h5 text-left")
    if not name_tag:
        continue

    name = name_tag.text.strip()
    snow_24hr = resort.find("span", class_="answer twentyfour").text.strip() \
                 if resort.find("span", class_="answer twentyfour") else "N/A"
    # ... repeat for other fields ...

    snow_data.append({
        "Date": datetime.now().strftime("%Y-%m-%d"),
        "Resort": name,
        "Snow (24hr)": snow_24hr,
        # ...
    })

new_data_df = pd.DataFrame(snow_data)

# Append or create CSV
if os.path.exists(csv_file_path):
    existing_data_df = pd.read_csv(csv_file_path)
    combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)
else:
    combined_df = new_data_df

combined_df.to_csv(csv_file_path, index=False)
print(new_data_df)
</code></pre>

        <h3>OnTheSnow Scraper</h3>
<pre><code>
import os
import re
import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

driver_path = r"C:\path\to\chromedriver.exe"
csv_file_path = r"C:\path\to\snow_report_data_onthesnow.csv"
log_file_path = r"C:\path\to\scraper_log.txt"

logging.basicConfig(filename=log_file_path, level=logging.INFO)

service = Service(driver_path)
driver = webdriver.Chrome(service=service)
driver.get("https://www.onthesnow.com/colorado/skireport")

WebDriverWait(driver, 40).until(
    EC.presence_of_element_located((By.CLASS_NAME, "styles_row__HA9Yq"))
)

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
driver.quit()

snow_data = []
resorts = soup.find_all("tr", class_="styles_row__HA9Yq")

for resort in resorts:
    data_tags = resort.find_all("span", class_="h4 styles_h4__x3zzi")
    # ... parse resort name, snowfall, base depth, open trails, etc. ...

new_data_df = pd.DataFrame(snow_data)

# Example of splitting base depth, computing % trails open, etc.
# ...
new_data_df.to_csv(csv_file_path, index=False)
logging.info("Data saved.")
print(new_data_df)
</code></pre>

        <h2>GitHub Repository</h2>
        <p>
            You can find the complete source code and instructions on how to run the web scraper in my 
            <a href="https://github.com/davo1176/davo1176.github.io/tree/main/Projects/Snow%20Report%20Web%20Scraper/SkiWebScraping" target="_blank">GitHub profile</a>.
        </p>

        <h2>Sample Output</h2>
        <p>
            Below are examples of console outputs or CSV entries showing updated data for ski resorts:
        </p>
        <img src="images/webscraper_sample_output.png" alt="Web Scraper Sample Output">

    </section>

    <footer>
        <p>&copy; 2024 Daniel Volin</p>
    </footer>
</body>
</html>
