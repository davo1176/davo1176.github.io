{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this is to create a web scraping program which scrapes all information on the webpage and turns it into a database. \n",
    "\n",
    "https://www.coloradoski.com/snow-report/\n",
    "\n",
    "I would like the final product to resemble the following:\n",
    "\n",
    "Name of Hill\tVariable\tValue\tDay\n",
    "Granby Ranch\t24hr\t0\tMarch 15, 2025\n",
    "Granby Ranch\t48hr\t5\tMarch 15, 2025\n",
    "Granby Ranch\tLifts Open\t5/5\tMarch 15, 2025\n",
    "Granby Ranch\t24hr\t0\tMarch 16, 2025\n",
    "Granby Ranch\t48hr\t0\tMarch 16, 2025\n",
    "Granby Ranch\tLifts Open\t5/5\tMarch 16, 2025\n",
    "Eldora\t24hr\t0\tMarch 15, 2025\n",
    "Eldora\t48hr\t3\tMarch 15, 2025\n",
    "Eldora\tLifts Open\t10/10\tMarch 15, 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Set up daily scrape..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:119\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m offsets\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    122\u001b[0m     concat,\n\u001b[0;32m    123\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     qcut,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\api.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\eval.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_extension_array_dtype\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENGINES\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     PARSERS,\n\u001b[0;32m     18\u001b[0m     Expr,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize_string\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ensure_scope\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1081\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1204\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Extraction Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nname --> h3 class = \"h5 text-left\"<Arapahoe Basin</h3>\\n24hr --> div class = \"value\", <p>, <span class = \"answer twentyfour\">\"</span>\\n48hr --> div class = \"value\", <p>, <span class = \"answer fourtyeight\">6\"</span>\\nMid-Mt Depth --> div class = \"value\", <p>, <span class = \"answer mid-mtn\">51\"</span>\\nSurface Conditions --> div class = \"value\", <p>, <span class = \"Surface\">HT/HP</span>\\nLifts Open --> this one is tricky as it needs to extract 3 values. Value 1: <div class = \"value\", <p class = \"lifts-open\">, <span class = \"open\">9</span>,<span class >/</span>, <span class = \"total\">9</span>\\nGreen --> div class = \"value\", <p>, <span class = \"green-runs\">100%</span>\\nBlue --> div class = \"value\", <p>, <span class = \"blue-runs\">100%</span>\\nBlack --> div class = \"value\", <p>, <span class = \"diamond-runs\">100%</span>\\nDouble Black --> div class = \"value\", <p>, <span class = \"double-diamond-runs\">100%</span>\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "name --> h3 class = \"h5 text-left\"<Arapahoe Basin</h3>\n",
    "24hr --> div class = \"value\", <p>, <span class = \"answer twentyfour\">\"</span>\n",
    "48hr --> div class = \"value\", <p>, <span class = \"answer fourtyeight\">6\"</span>\n",
    "Mid-Mt Depth --> div class = \"value\", <p>, <span class = \"answer mid-mtn\">51\"</span>\n",
    "Surface Conditions --> div class = \"value\", <p>, <span class = \"Surface\">HT/HP</span>\n",
    "Lifts Open --> this one is tricky as it needs to extract 3 values. Value 1: <div class = \"value\", <p class = \"lifts-open\">, <span class = \"open\">9</span>,<span class >/</span>, <span class = \"total\">9</span>\n",
    "Green --> div class = \"value\", <p>, <span class = \"green-runs\">100%</span>\n",
    "Blue --> div class = \"value\", <p>, <span class = \"blue-runs\">100%</span>\n",
    "Black --> div class = \"value\", <p>, <span class = \"diamond-runs\">100%</span>\n",
    "Double Black --> div class = \"value\", <p>, <span class = \"double-diamond-runs\">100%</span>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML saved to snow_report_raw.html\n"
     ]
    }
   ],
   "source": [
    "## Investigation into the HTML code so I can learn to extract from it... \n",
    "\n",
    "url = \"https://www.coloradoski.com/snow-report/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://www.google.com/\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Save raw HTML to a file\n",
    "with open(\"snow_report_raw.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"HTML saved to snow_report_raw.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date             Resort Snow (24hr) Snow (48hr) Mid-Mt Depth  \\\n",
      "0   2025-02-09     Arapahoe Basin          2\"          8\"          52”   \n",
      "1   2025-02-09    Aspen Highlands          7\"         11\"          41”   \n",
      "2   2025-02-09     Aspen Mountain          9\"         11\"          37”   \n",
      "3   2025-02-09         Buttermilk          6\"          7\"          30”   \n",
      "4   2025-02-09             Cooper          3\"          8\"          37”   \n",
      "5   2025-02-09    Copper Mountain          6\"         10\"          57”   \n",
      "6   2025-02-09      Echo Mountain          0\"          0\"          18”   \n",
      "7   2025-02-09             Eldora          6\"          9\"          30”   \n",
      "8   2025-02-09       Granby Ranch          1\"          5\"          32”   \n",
      "9   2025-02-09      Howelsen Hill          0\"          0\"          34”   \n",
      "10  2025-02-09  Loveland Ski Area          4\"          8\"          44”   \n",
      "11  2025-02-09            Monarch          0\"          0\"          41”   \n",
      "12  2025-02-09         Powderhorn          2\"          3\"          35”   \n",
      "13  2025-02-09   Purgatory Resort          0\"          0\"          33”   \n",
      "14  2025-02-09          Silverton          0\"          0\"          36”   \n",
      "15  2025-02-09           Snowmass          9\"         11\"          43”   \n",
      "16  2025-02-09          Steamboat          5\"         15\"          53”   \n",
      "17  2025-02-09           Sunlight          2\"          4\"          25”   \n",
      "18  2025-02-09          Telluride          0\"          0\"          37”   \n",
      "19  2025-02-09        Winter Park         11\"         16\"          56”   \n",
      "\n",
      "   Surface Conditions Lifts Open Green Runs Blue Runs Black Runs  \\\n",
      "0               HP/PP        9/9       100%      100%       100%   \n",
      "1                  PP        5/5        N/A      100%       100%   \n",
      "2                  PP        7/8        N/A      100%       100%   \n",
      "3                  PP        5/8       100%      100%        89%   \n",
      "4                P/PP        5/5       100%      100%       100%   \n",
      "5                   P      24/24       100%      100%        97%   \n",
      "6                  MM        2/2        67%      100%         0%   \n",
      "7               PP/HP      10/10       100%       97%       100%   \n",
      "8                  PP        5/5       100%      100%       100%   \n",
      "9               MM/SP        3/3       100%       88%       100%   \n",
      "10               P/PP      10/10       100%       98%       100%   \n",
      "11              HP/PP        7/7       100%      100%       100%   \n",
      "12              PP/HP        5/5       100%       94%        89%   \n",
      "13               PP/P      10/11        88%      100%        88%   \n",
      "14                           1/1        N/A       N/A        N/A   \n",
      "15                 PP      17/20       100%      100%       100%   \n",
      "16                  P      23/23       100%      100%       100%   \n",
      "17              SP/MM        3/3       100%      100%        54%   \n",
      "18                 PP      17/17       100%      100%        97%   \n",
      "19               P/PP      22/23       100%      100%        99%   \n",
      "\n",
      "   Double Black Runs  \n",
      "0                79%  \n",
      "1                84%  \n",
      "2                96%  \n",
      "3                 0%  \n",
      "4               100%  \n",
      "5               100%  \n",
      "6                 0%  \n",
      "7                90%  \n",
      "8                N/A  \n",
      "9                N/A  \n",
      "10              100%  \n",
      "11               88%  \n",
      "12               50%  \n",
      "13               58%  \n",
      "14              100%  \n",
      "15               92%  \n",
      "16              100%  \n",
      "17               15%  \n",
      "18               71%  \n",
      "19               58%  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "driver_path = \"C:/Users/danie/chromedriver-win64/chromedriver.exe\"\n",
    "csv_file_path = \"C:/Users/danie/OneDrive/Desktop/PersonalProjects/SkiWebScraping/snow_report_data_soloradoski.csv\"\n",
    "\n",
    "# Set up the Chrome driver using Service\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the website\n",
    "url = \"https://www.coloradoski.com/snow-report/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Dynamic wait: Wait until at least one Mid-Mt Depth value is not \"0\"\n",
    "try:\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        lambda d: any(\n",
    "            mid_mt.text.strip() != '0\"'\n",
    "            for mid_mt in d.find_elements(By.CLASS_NAME, \"answer.mid-mtn\")\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Timeout waiting for Mid-Mt Depth to load:\", e)\n",
    "    driver.quit()\n",
    "\n",
    "# Get page source after JavaScript has loaded\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Initialize list to hold data\n",
    "snow_data = []\n",
    "\n",
    "# Extract resort data\n",
    "resorts = soup.find_all(\"div\", class_=\"inner\")\n",
    "\n",
    "for resort in resorts:\n",
    "    # Extract Resort Name\n",
    "    name_tag = resort.find(\"h3\", class_=\"h5 text-left\")\n",
    "    \n",
    "    # Extract Snow-related Data\n",
    "    snow_24hr_tag = resort.find(\"span\", class_=\"answer twentyfour\")\n",
    "    snow_48hr_tag = resort.find(\"span\", class_=\"answer fortyeight\")\n",
    "    mid_mt_depth_tag = resort.find(\"span\", class_=\"answer mid-mtn\")\n",
    "    surface_conditions_tag = resort.find(\"p\", class_=\"surface\")\n",
    "    lifts_open_tag = resort.find(\"p\", class_=\"lifts-open\")\n",
    "    green_runs_tag = resort.find(\"p\", class_=\"green-runs\")\n",
    "    blue_runs_tag = resort.find(\"p\", class_=\"blue-runs\")\n",
    "    black_runs_tag = resort.find(\"p\", class_=\"diamond-runs\")\n",
    "    double_black_runs_tag = resort.find(\"p\", class_=\"double-diamond-runs\")\n",
    "\n",
    "    # Ensure Resort Name exists\n",
    "    if name_tag:\n",
    "        name = name_tag.text.strip()\n",
    "        snow_24hr = snow_24hr_tag.text.strip() if snow_24hr_tag else \"N/A\"\n",
    "        snow_48hr = snow_48hr_tag.text.strip() if snow_48hr_tag else \"N/A\"\n",
    "        mid_mt_depth = mid_mt_depth_tag.text.strip() if mid_mt_depth_tag else \"N/A\"\n",
    "        surface_conditions = surface_conditions_tag.text.strip() if surface_conditions_tag else \"N/A\"\n",
    "        lifts_open = lifts_open_tag.text.strip() if lifts_open_tag else \"N/A\"\n",
    "        green_runs = green_runs_tag.text.strip() if green_runs_tag else \"N/A\"\n",
    "        blue_runs = blue_runs_tag.text.strip() if blue_runs_tag else \"N/A\"\n",
    "        black_runs = black_runs_tag.text.strip() if black_runs_tag else \"N/A\"\n",
    "        double_black_runs = double_black_runs_tag.text.strip() if double_black_runs_tag else \"N/A\"\n",
    "\n",
    "        # Append data\n",
    "        snow_data.append({\n",
    "            \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"Resort\": name,\n",
    "            \"Snow (24hr)\": snow_24hr,\n",
    "            \"Snow (48hr)\": snow_48hr,\n",
    "            \"Mid-Mt Depth\": mid_mt_depth,\n",
    "            \"Surface Conditions\": surface_conditions,\n",
    "            \"Lifts Open\": lifts_open,\n",
    "            \"Green Runs\": green_runs,\n",
    "            \"Blue Runs\": blue_runs,\n",
    "            \"Black Runs\": black_runs,\n",
    "            \"Double Black Runs\": double_black_runs\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_data_df = pd.DataFrame(snow_data)\n",
    "\n",
    "# Check if the CSV already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Read existing data\n",
    "    existing_data_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Append new data to existing data\n",
    "    combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)\n",
    "else:\n",
    "    # If CSV doesn't exist, the new data becomes the initial data\n",
    "    combined_df = new_data_df\n",
    "\n",
    "# Save the combined DataFrame back to CSV\n",
    "combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Display the newly added data\n",
    "print(new_data_df)\n",
    "new_data_df_coloradoski =  new_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_df_coloradoski =  new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Resort</th>\n",
       "      <th>Snow (24hr)</th>\n",
       "      <th>Snow (48hr)</th>\n",
       "      <th>Mid-Mt Depth</th>\n",
       "      <th>Surface Conditions</th>\n",
       "      <th>Lifts Open</th>\n",
       "      <th>Green Runs</th>\n",
       "      <th>Blue Runs</th>\n",
       "      <th>Black Runs</th>\n",
       "      <th>Double Black Runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Arapahoe Basin</td>\n",
       "      <td>2\"</td>\n",
       "      <td>8\"</td>\n",
       "      <td>52”</td>\n",
       "      <td>HP/PP</td>\n",
       "      <td>9/9</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Aspen Highlands</td>\n",
       "      <td>7\"</td>\n",
       "      <td>11\"</td>\n",
       "      <td>41”</td>\n",
       "      <td>PP</td>\n",
       "      <td>5/5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Aspen Mountain</td>\n",
       "      <td>9\"</td>\n",
       "      <td>11\"</td>\n",
       "      <td>37”</td>\n",
       "      <td>PP</td>\n",
       "      <td>7/8</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Buttermilk</td>\n",
       "      <td>6\"</td>\n",
       "      <td>7\"</td>\n",
       "      <td>30”</td>\n",
       "      <td>PP</td>\n",
       "      <td>5/8</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>89%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Cooper</td>\n",
       "      <td>3\"</td>\n",
       "      <td>8\"</td>\n",
       "      <td>37”</td>\n",
       "      <td>P/PP</td>\n",
       "      <td>5/5</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Copper Mountain</td>\n",
       "      <td>6\"</td>\n",
       "      <td>10\"</td>\n",
       "      <td>57”</td>\n",
       "      <td>P</td>\n",
       "      <td>24/24</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>97%</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Echo Mountain</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>18”</td>\n",
       "      <td>MM</td>\n",
       "      <td>2/2</td>\n",
       "      <td>67%</td>\n",
       "      <td>100%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Eldora</td>\n",
       "      <td>6\"</td>\n",
       "      <td>9\"</td>\n",
       "      <td>30”</td>\n",
       "      <td>PP/HP</td>\n",
       "      <td>10/10</td>\n",
       "      <td>100%</td>\n",
       "      <td>97%</td>\n",
       "      <td>100%</td>\n",
       "      <td>90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Granby Ranch</td>\n",
       "      <td>1\"</td>\n",
       "      <td>5\"</td>\n",
       "      <td>32”</td>\n",
       "      <td>PP</td>\n",
       "      <td>5/5</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Howelsen Hill</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>34”</td>\n",
       "      <td>MM/SP</td>\n",
       "      <td>3/3</td>\n",
       "      <td>100%</td>\n",
       "      <td>88%</td>\n",
       "      <td>100%</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Loveland Ski Area</td>\n",
       "      <td>4\"</td>\n",
       "      <td>8\"</td>\n",
       "      <td>44”</td>\n",
       "      <td>P/PP</td>\n",
       "      <td>10/10</td>\n",
       "      <td>100%</td>\n",
       "      <td>98%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Monarch</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>41”</td>\n",
       "      <td>HP/PP</td>\n",
       "      <td>7/7</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Powderhorn</td>\n",
       "      <td>2\"</td>\n",
       "      <td>3\"</td>\n",
       "      <td>35”</td>\n",
       "      <td>PP/HP</td>\n",
       "      <td>5/5</td>\n",
       "      <td>100%</td>\n",
       "      <td>94%</td>\n",
       "      <td>89%</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Purgatory Resort</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>33”</td>\n",
       "      <td>PP/P</td>\n",
       "      <td>10/11</td>\n",
       "      <td>88%</td>\n",
       "      <td>100%</td>\n",
       "      <td>88%</td>\n",
       "      <td>58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Silverton</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>36”</td>\n",
       "      <td></td>\n",
       "      <td>1/1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Snowmass</td>\n",
       "      <td>9\"</td>\n",
       "      <td>11\"</td>\n",
       "      <td>43”</td>\n",
       "      <td>PP</td>\n",
       "      <td>17/20</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Steamboat</td>\n",
       "      <td>5\"</td>\n",
       "      <td>15\"</td>\n",
       "      <td>53”</td>\n",
       "      <td>P</td>\n",
       "      <td>23/23</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Sunlight</td>\n",
       "      <td>2\"</td>\n",
       "      <td>4\"</td>\n",
       "      <td>25”</td>\n",
       "      <td>SP/MM</td>\n",
       "      <td>3/3</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>54%</td>\n",
       "      <td>15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Telluride</td>\n",
       "      <td>0\"</td>\n",
       "      <td>0\"</td>\n",
       "      <td>37”</td>\n",
       "      <td>PP</td>\n",
       "      <td>17/17</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>97%</td>\n",
       "      <td>71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Winter Park</td>\n",
       "      <td>11\"</td>\n",
       "      <td>16\"</td>\n",
       "      <td>56”</td>\n",
       "      <td>P/PP</td>\n",
       "      <td>22/23</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>99%</td>\n",
       "      <td>58%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date             Resort Snow (24hr) Snow (48hr) Mid-Mt Depth  \\\n",
       "0   2025-02-09     Arapahoe Basin          2\"          8\"          52”   \n",
       "1   2025-02-09    Aspen Highlands          7\"         11\"          41”   \n",
       "2   2025-02-09     Aspen Mountain          9\"         11\"          37”   \n",
       "3   2025-02-09         Buttermilk          6\"          7\"          30”   \n",
       "4   2025-02-09             Cooper          3\"          8\"          37”   \n",
       "5   2025-02-09    Copper Mountain          6\"         10\"          57”   \n",
       "6   2025-02-09      Echo Mountain          0\"          0\"          18”   \n",
       "7   2025-02-09             Eldora          6\"          9\"          30”   \n",
       "8   2025-02-09       Granby Ranch          1\"          5\"          32”   \n",
       "9   2025-02-09      Howelsen Hill          0\"          0\"          34”   \n",
       "10  2025-02-09  Loveland Ski Area          4\"          8\"          44”   \n",
       "11  2025-02-09            Monarch          0\"          0\"          41”   \n",
       "12  2025-02-09         Powderhorn          2\"          3\"          35”   \n",
       "13  2025-02-09   Purgatory Resort          0\"          0\"          33”   \n",
       "14  2025-02-09          Silverton          0\"          0\"          36”   \n",
       "15  2025-02-09           Snowmass          9\"         11\"          43”   \n",
       "16  2025-02-09          Steamboat          5\"         15\"          53”   \n",
       "17  2025-02-09           Sunlight          2\"          4\"          25”   \n",
       "18  2025-02-09          Telluride          0\"          0\"          37”   \n",
       "19  2025-02-09        Winter Park         11\"         16\"          56”   \n",
       "\n",
       "   Surface Conditions Lifts Open Green Runs Blue Runs Black Runs  \\\n",
       "0               HP/PP        9/9       100%      100%       100%   \n",
       "1                  PP        5/5        N/A      100%       100%   \n",
       "2                  PP        7/8        N/A      100%       100%   \n",
       "3                  PP        5/8       100%      100%        89%   \n",
       "4                P/PP        5/5       100%      100%       100%   \n",
       "5                   P      24/24       100%      100%        97%   \n",
       "6                  MM        2/2        67%      100%         0%   \n",
       "7               PP/HP      10/10       100%       97%       100%   \n",
       "8                  PP        5/5       100%      100%       100%   \n",
       "9               MM/SP        3/3       100%       88%       100%   \n",
       "10               P/PP      10/10       100%       98%       100%   \n",
       "11              HP/PP        7/7       100%      100%       100%   \n",
       "12              PP/HP        5/5       100%       94%        89%   \n",
       "13               PP/P      10/11        88%      100%        88%   \n",
       "14                           1/1        N/A       N/A        N/A   \n",
       "15                 PP      17/20       100%      100%       100%   \n",
       "16                  P      23/23       100%      100%       100%   \n",
       "17              SP/MM        3/3       100%      100%        54%   \n",
       "18                 PP      17/17       100%      100%        97%   \n",
       "19               P/PP      22/23       100%      100%        99%   \n",
       "\n",
       "   Double Black Runs  \n",
       "0                79%  \n",
       "1                84%  \n",
       "2                96%  \n",
       "3                 0%  \n",
       "4               100%  \n",
       "5               100%  \n",
       "6                 0%  \n",
       "7                90%  \n",
       "8                N/A  \n",
       "9                N/A  \n",
       "10              100%  \n",
       "11               88%  \n",
       "12               50%  \n",
       "13               58%  \n",
       "14              100%  \n",
       "15               92%  \n",
       "16              100%  \n",
       "17               15%  \n",
       "18               71%  \n",
       "19               58%  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Still missing: \n",
    "1. Beaver Creek\n",
    "2. Breckenridge\n",
    "3. Crested Butte\n",
    "4. Keystone\n",
    "5. Ski Granby Ranch\n",
    "6. Silverton Mountain\n",
    "7. Vail\n",
    "8. Wolf Creek Ski Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML saved to snow_report_raw.html\n"
     ]
    }
   ],
   "source": [
    "## Investigation into the HTML code so I can learn to extract from it... \n",
    "\n",
    "url = \"https://www.onthesnow.com/colorado/skireport\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://www.google.com/\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Save raw HTML to a file\n",
    "with open(\"onthesnow_report_raw.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"HTML saved to snow_report_raw.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date                         Resort Snow (24hr) Base Depth  \\\n",
      "0   2025-02-09        Arapahoe Basin Ski Area          2\"        52\"   \n",
      "1   2025-02-09                 Aspen Snowmass          9\"        53\"   \n",
      "2   2025-02-09                   Beaver Creek          2\"        44\"   \n",
      "3   2025-02-09                   Breckenridge          7\"        53\"   \n",
      "4   2025-02-09                         Cooper          3\"        40\"   \n",
      "5   2025-02-09                Copper Mountain          6\"        58\"   \n",
      "6   2025-02-09  Crested Butte Mountain Resort          0\"        45\"   \n",
      "7   2025-02-09                  Echo Mountain          0\"        30\"   \n",
      "8   2025-02-09         Eldora Mountain Resort          6\"        30\"   \n",
      "9   2025-02-09                  Howelsen Hill          0\"        34\"   \n",
      "10  2025-02-09                       Keystone          3\"        43\"   \n",
      "11  2025-02-09              Loveland Ski Area          4\"        46\"   \n",
      "12  2025-02-09               Monarch Mountain          0\"        41\"   \n",
      "13  2025-02-09                     Powderhorn          0\"        35\"   \n",
      "14  2025-02-09                      Purgatory          0\"        35\"   \n",
      "15  2025-02-09             Silverton Mountain          0\"        57\"   \n",
      "16  2025-02-09               Ski Granby Ranch          1\"        72\"   \n",
      "17  2025-02-09                      Steamboat          5\"        84\"   \n",
      "18  2025-02-09       Sunlight Mountain Resort          2\"        28\"   \n",
      "19  2025-02-09                      Telluride          0\"        37\"   \n",
      "20  2025-02-09                           Vail          4\"        54\"   \n",
      "21  2025-02-09                    Winter Park         13\"        65\"   \n",
      "22  2025-02-09            Wolf Creek Ski Area          0\"        45\"   \n",
      "\n",
      "    Trails Open  Total Trails Lifts Open     Base Depth Notes  % Trails Open  \n",
      "0           133           145        8/9  Variable Conditions          91.72  \n",
      "1           349           366      34/41               Powder          95.36  \n",
      "2           168           173      18/24      Machine Groomed          97.11  \n",
      "3           188           188      35/35               Powder         100.00  \n",
      "4            64            64        5/5               Powder         100.00  \n",
      "5           148           150      24/24               Powder          98.67  \n",
      "6           120           165      15/15      Machine Groomed          72.73  \n",
      "7             7             7        3/3  Variable Conditions         100.00  \n",
      "8            60            61      10/10               Powder          98.36  \n",
      "9            12            17        3/4      Machine Groomed          70.59  \n",
      "10          140           140      21/21               Powder         100.00  \n",
      "11           93            94      10/10               Powder          98.94  \n",
      "12           66            67        7/7        Packed Powder          98.51  \n",
      "13           49            54        5/5      Machine Groomed          90.74  \n",
      "14           94           107      10/11        Packed Powder          87.85  \n",
      "15           69            69        1/1               Powder         100.00  \n",
      "16           42            42        5/5      Machine Groomed         100.00  \n",
      "17          184           184      23/23               Powder         100.00  \n",
      "18           45            75        3/3  Variable Conditions          60.00  \n",
      "19          129           147      17/17        Packed Powder          87.76  \n",
      "20          274           278      30/33               Powder          98.56  \n",
      "21          160           171      23/25        Packed Powder          93.57  \n",
      "22          144           144      10/11      Machine Groomed         100.00  \n"
     ]
    }
   ],
   "source": [
    "# Note: Takes the Higher Base Depth Value if a Range is Given\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# File paths\n",
    "driver_path = r\"C:\\Users\\danie\\chromedriver-win64\\chromedriver.exe\"\n",
    "csv_file_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\snow_report_data_onthesnow.csv\"\n",
    "log_file_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\scraper_log.txt\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.info(\"Script started.\")\n",
    "\n",
    "try:\n",
    "    # Set up the Chrome driver using Service\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Open the website\n",
    "    url = \"https://www.onthesnow.com/colorado/skireport\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Dynamic wait: Wait until resort rows are loaded\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"styles_row__HA9Yq\"))\n",
    "    )\n",
    "    logging.info(\"Resort data loaded successfully.\")\n",
    "\n",
    "    # Get page source after JavaScript has loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Initialize list to hold data\n",
    "    snow_data = []\n",
    "\n",
    "    # Extract resort data\n",
    "    resorts = soup.find_all(\"tr\", class_=\"styles_row__HA9Yq\")  # Each resort is in a table row\n",
    "\n",
    "    for resort in resorts:\n",
    "        data_tags = resort.find_all(\"span\", class_=\"h4 styles_h4__x3zzi\")\n",
    "\n",
    "        # Ensure enough data points exist\n",
    "        if len(data_tags) >= 5:\n",
    "            open_trails_raw = data_tags[3].text.strip() if data_tags[3] else \"N/A\"\n",
    "\n",
    "            # Regex to extract Trails Open, Total Trails, and Percentage Open\n",
    "            match = re.search(r\"(\\d+)\\s*/\\s*(\\d+)(\\d{2,3})?\\s*(\\d+%)?\", open_trails_raw)\n",
    "\n",
    "            if match:\n",
    "                trails_open = int(match.group(1))\n",
    "                total_trails_raw = match.group(2)\n",
    "                last_digits = int(match.group(3)) if match.group(3) else \"N/A\"\n",
    "                percentage_open = match.group(4) if match.group(4) else \"N/A\"\n",
    "\n",
    "                # Correction logic\n",
    "                total_trails = int(total_trails_raw)\n",
    "                corrected_total_trails = total_trails\n",
    "\n",
    "                # Attempt to correct by removing trailing digits\n",
    "                for i in range(1, 4):  # Check removing 1 to 3 digits\n",
    "                    possible_total = int(str(total_trails)[:-i]) if len(str(total_trails)) > i else total_trails\n",
    "\n",
    "                    if possible_total >= trails_open:\n",
    "                        if percentage_open != \"N/A\":\n",
    "                            calculated_percentage = round((trails_open / possible_total) * 100)\n",
    "                            if calculated_percentage == int(percentage_open.strip('%')):\n",
    "                                corrected_total_trails = possible_total\n",
    "                                break\n",
    "                        else:\n",
    "                            # If percentage is not provided, use logical deduction\n",
    "                            if possible_total - trails_open <= 50:  # Assuming it's reasonable\n",
    "                                corrected_total_trails = possible_total\n",
    "                                break\n",
    "\n",
    "                # Final validation\n",
    "                if corrected_total_trails < trails_open:\n",
    "                    corrected_total_trails = \"N/A\"\n",
    "\n",
    "            else:\n",
    "                trails_open, total_trails_raw, corrected_total_trails, percentage_open = \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "            snow_data.append({\n",
    "                \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                \"Resort\": data_tags[0].text.strip() if data_tags[0] else \"N/A\",          # Resort Name\n",
    "                \"Snow (24hr)\": data_tags[1].text.strip().rstrip('\"').rstrip('-') if data_tags[1] else \"N/A\",      # Cleaned Snow (24hr)\n",
    "                \"Base Depth\": data_tags[2].text.strip() if data_tags[2] else \"N/A\",       # Base Depth\n",
    "                \"Trails Open\": trails_open,                                               # Cleaned Trails Open\n",
    "                \"Total Trails\": corrected_total_trails,                                   # Renamed Corrected Total Trails\n",
    "                \"Lifts Open\": data_tags[4].text.strip().rstrip('-') if data_tags[4] else \"N/A\"        # Cleaned Lifts Open\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_data_df = pd.DataFrame(snow_data)\n",
    "\n",
    "    # Separate Base Depth into two columns\n",
    "    new_data_df[['Base Depth', 'Base Depth Notes']] = new_data_df['Base Depth'].str.extract(r'(\\d+\\\"(?:-\\d+\\\")?)(.*)')\n",
    "\n",
    "    # Calculate % Trails Open\n",
    "    new_data_df[\"% Trails Open\"] = (new_data_df[\"Trails Open\"] / new_data_df[\"Total Trails\"] * 100).round(2)\n",
    "\n",
    "    # Append or create CSV\n",
    "    if os.path.exists(csv_file_path):\n",
    "        existing_data_df = pd.read_csv(csv_file_path)\n",
    "        combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_data_df\n",
    "\n",
    "    # Save combined data\n",
    "    combined_df.to_csv(csv_file_path, index=False)\n",
    "    logging.info(f\"Data successfully saved to {csv_file_path}\")\n",
    "\n",
    "    # Display newly added data\n",
    "    print(new_data_df)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First Script -- Colorado Ski\n",
    "'''\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "driver_path = \"C:/Users/danie/chromedriver-win64/chromedriver.exe\"\n",
    "csv_file_path = \"C:/Users/danie/OneDrive/Desktop/PersonalProjects/SkiWebScraping/snow_report_data_soloradoski.csv\"\n",
    "\n",
    "# Set up the Chrome driver using Service\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the website\n",
    "url = \"https://www.coloradoski.com/snow-report/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Dynamic wait: Wait until at least one Mid-Mt Depth value is not \"0\"\n",
    "try:\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        lambda d: any(\n",
    "            mid_mt.text.strip() != '0\"'\n",
    "            for mid_mt in d.find_elements(By.CLASS_NAME, \"answer.mid-mtn\")\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Timeout waiting for Mid-Mt Depth to load:\", e)\n",
    "    driver.quit()\n",
    "\n",
    "# Get page source after JavaScript has loaded\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Initialize list to hold data\n",
    "snow_data = []\n",
    "\n",
    "# Extract resort data\n",
    "resorts = soup.find_all(\"div\", class_=\"inner\")\n",
    "\n",
    "for resort in resorts:\n",
    "    # Extract Resort Name\n",
    "    name_tag = resort.find(\"h3\", class_=\"h5 text-left\")\n",
    "    \n",
    "    # Extract Snow-related Data\n",
    "    snow_24hr_tag = resort.find(\"span\", class_=\"answer twentyfour\")\n",
    "    snow_48hr_tag = resort.find(\"span\", class_=\"answer fortyeight\")\n",
    "    mid_mt_depth_tag = resort.find(\"span\", class_=\"answer mid-mtn\")\n",
    "    surface_conditions_tag = resort.find(\"p\", class_=\"surface\")\n",
    "    lifts_open_tag = resort.find(\"p\", class_=\"lifts-open\")\n",
    "    green_runs_tag = resort.find(\"p\", class_=\"green-runs\")\n",
    "    blue_runs_tag = resort.find(\"p\", class_=\"blue-runs\")\n",
    "    black_runs_tag = resort.find(\"p\", class_=\"diamond-runs\")\n",
    "    double_black_runs_tag = resort.find(\"p\", class_=\"double-diamond-runs\")\n",
    "\n",
    "    # Ensure Resort Name exists\n",
    "    if name_tag:\n",
    "        name = name_tag.text.strip()\n",
    "        snow_24hr = snow_24hr_tag.text.strip() if snow_24hr_tag else \"N/A\"\n",
    "        snow_48hr = snow_48hr_tag.text.strip() if snow_48hr_tag else \"N/A\"\n",
    "        mid_mt_depth = mid_mt_depth_tag.text.strip() if mid_mt_depth_tag else \"N/A\"\n",
    "        surface_conditions = surface_conditions_tag.text.strip() if surface_conditions_tag else \"N/A\"\n",
    "        lifts_open = lifts_open_tag.text.strip() if lifts_open_tag else \"N/A\"\n",
    "        green_runs = green_runs_tag.text.strip() if green_runs_tag else \"N/A\"\n",
    "        blue_runs = blue_runs_tag.text.strip() if blue_runs_tag else \"N/A\"\n",
    "        black_runs = black_runs_tag.text.strip() if black_runs_tag else \"N/A\"\n",
    "        double_black_runs = double_black_runs_tag.text.strip() if double_black_runs_tag else \"N/A\"\n",
    "\n",
    "        # Append data\n",
    "        snow_data.append({\n",
    "            \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"Resort\": name,\n",
    "            \"Snow (24hr)\": snow_24hr,\n",
    "            \"Snow (48hr)\": snow_48hr,\n",
    "            \"Mid-Mt Depth\": mid_mt_depth,\n",
    "            \"Surface Conditions\": surface_conditions,\n",
    "            \"Lifts Open\": lifts_open,\n",
    "            \"Green Runs\": green_runs,\n",
    "            \"Blue Runs\": blue_runs,\n",
    "            \"Black Runs\": black_runs,\n",
    "            \"Double Black Runs\": double_black_runs\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_data_df = pd.DataFrame(snow_data)\n",
    "\n",
    "# Check if the CSV already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Read existing data\n",
    "    existing_data_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Append new data to existing data\n",
    "    combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)\n",
    "else:\n",
    "    # If CSV doesn't exist, the new data becomes the initial data\n",
    "    combined_df = new_data_df\n",
    "\n",
    "# Save the combined DataFrame back to CSV\n",
    "combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Display the newly added data\n",
    "print(new_data_df)\n",
    "\n",
    "\n",
    "'''\n",
    "Second Script -- OnTheSnow\n",
    "'''\n",
    "# Note: Takes the Higher Base Depth Value if a Range is Given\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# File paths\n",
    "driver_path = r\"C:\\Users\\danie\\chromedriver-win64\\chromedriver.exe\"\n",
    "csv_file_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\snow_report_data_onthesnow.csv\"\n",
    "log_file_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\scraper_log.txt\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.info(\"Script started.\")\n",
    "\n",
    "try:\n",
    "    # Set up the Chrome driver using Service\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Open the website\n",
    "    url = \"https://www.onthesnow.com/colorado/skireport\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Dynamic wait: Wait until resort rows are loaded\n",
    "    WebDriverWait(driver, 40).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"styles_row__HA9Yq\"))\n",
    "    )\n",
    "    logging.info(\"Resort data loaded successfully.\")\n",
    "\n",
    "    # Get page source after JavaScript has loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Initialize list to hold data\n",
    "    snow_data = []\n",
    "\n",
    "    # Extract resort data\n",
    "    resorts = soup.find_all(\"tr\", class_=\"styles_row__HA9Yq\")  # Each resort is in a table row\n",
    "\n",
    "    for resort in resorts:\n",
    "        data_tags = resort.find_all(\"span\", class_=\"h4 styles_h4__x3zzi\")\n",
    "\n",
    "        # Ensure enough data points exist\n",
    "        if len(data_tags) >= 5:\n",
    "            open_trails_raw = data_tags[3].text.strip() if data_tags[3] else \"N/A\"\n",
    "\n",
    "            # Regex to extract Trails Open, Total Trails, and Percentage Open\n",
    "            match = re.search(r\"(\\d+)\\s*/\\s*(\\d+)(\\d{2,3})?\\s*(\\d+%)?\", open_trails_raw)\n",
    "\n",
    "            if match:\n",
    "                trails_open = int(match.group(1))\n",
    "                total_trails_raw = match.group(2)\n",
    "                last_digits = int(match.group(3)) if match.group(3) else \"N/A\"\n",
    "                percentage_open = match.group(4) if match.group(4) else \"N/A\"\n",
    "\n",
    "                # Correction logic\n",
    "                total_trails = int(total_trails_raw)\n",
    "                corrected_total_trails = total_trails\n",
    "\n",
    "                # Attempt to correct by removing trailing digits\n",
    "                for i in range(1, 4):  # Check removing 1 to 3 digits\n",
    "                    possible_total = int(str(total_trails)[:-i]) if len(str(total_trails)) > i else total_trails\n",
    "\n",
    "                    if possible_total >= trails_open:\n",
    "                        if percentage_open != \"N/A\":\n",
    "                            calculated_percentage = round((trails_open / possible_total) * 100)\n",
    "                            if calculated_percentage == int(percentage_open.strip('%')):\n",
    "                                corrected_total_trails = possible_total\n",
    "                                break\n",
    "                        else:\n",
    "                            # If percentage is not provided, use logical deduction\n",
    "                            if possible_total - trails_open <= 50:  # Assuming it's reasonable\n",
    "                                corrected_total_trails = possible_total\n",
    "                                break\n",
    "\n",
    "                # Final validation\n",
    "                if corrected_total_trails < trails_open:\n",
    "                    corrected_total_trails = \"N/A\"\n",
    "\n",
    "            else:\n",
    "                trails_open, total_trails_raw, corrected_total_trails, percentage_open = \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "            snow_data.append({\n",
    "                \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                \"Resort\": data_tags[0].text.strip() if data_tags[0] else \"N/A\",          # Resort Name\n",
    "                \"Snow (24hr)\": data_tags[1].text.strip().rstrip('\"').rstrip('-') if data_tags[1] else \"N/A\",      # Cleaned Snow (24hr)\n",
    "                \"Base Depth\": data_tags[2].text.strip() if data_tags[2] else \"N/A\",       # Base Depth\n",
    "                \"Trails Open\": trails_open,                                               # Cleaned Trails Open\n",
    "                \"Total Trails\": corrected_total_trails,                                   # Renamed Corrected Total Trails\n",
    "                \"Lifts Open\": data_tags[4].text.strip().rstrip('-') if data_tags[4] else \"N/A\"        # Cleaned Lifts Open\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_data_df = pd.DataFrame(snow_data)\n",
    "\n",
    "    # Separate Base Depth into two columns\n",
    "    new_data_df[['Base Depth', 'Base Depth Notes']] = new_data_df['Base Depth'].str.extract(r'(\\d+\\\"(?:-\\d+\\\")?)(.*)')\n",
    "\n",
    "    # Calculate % Trails Open\n",
    "    new_data_df[\"% Trails Open\"] = (new_data_df[\"Trails Open\"] / new_data_df[\"Total Trails\"] * 100).round(2)\n",
    "\n",
    "    # Append or create CSV\n",
    "    if os.path.exists(csv_file_path):\n",
    "        existing_data_df = pd.read_csv(csv_file_path)\n",
    "        combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_data_df\n",
    "\n",
    "    # Save combined data\n",
    "    combined_df.to_csv(csv_file_path, index=False)\n",
    "    logging.info(f\"Data successfully saved to {csv_file_path}\")\n",
    "\n",
    "    # Display newly added data\n",
    "    print(new_data_df)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Resort</th>\n",
       "      <th>Snow (24hr)</th>\n",
       "      <th>Snow (48hr)</th>\n",
       "      <th>Mid-Mt Depth</th>\n",
       "      <th>Surface Conditions</th>\n",
       "      <th>Lifts Open</th>\n",
       "      <th>Green Runs</th>\n",
       "      <th>Blue Runs</th>\n",
       "      <th>Black Runs</th>\n",
       "      <th>Double Black Runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Arapahoe Basin</td>\n",
       "      <td>2\"</td>\n",
       "      <td>8\"</td>\n",
       "      <td>52”</td>\n",
       "      <td>HP/PP</td>\n",
       "      <td>9/9</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Aspen Highlands</td>\n",
       "      <td>7\"</td>\n",
       "      <td>11\"</td>\n",
       "      <td>41”</td>\n",
       "      <td>PP</td>\n",
       "      <td>5/5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>84%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date           Resort Snow (24hr) Snow (48hr) Mid-Mt Depth  \\\n",
       "0  2025-02-09   Arapahoe Basin          2\"          8\"          52”   \n",
       "1  2025-02-09  Aspen Highlands          7\"         11\"          41”   \n",
       "\n",
       "  Surface Conditions Lifts Open Green Runs Blue Runs Black Runs  \\\n",
       "0              HP/PP        9/9       100%      100%       100%   \n",
       "1                 PP        5/5        N/A      100%       100%   \n",
       "\n",
       "  Double Black Runs  \n",
       "0               79%  \n",
       "1               84%  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df_coloradoski.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Resort</th>\n",
       "      <th>Snow (24hr)</th>\n",
       "      <th>Base Depth</th>\n",
       "      <th>Trails Open</th>\n",
       "      <th>Total Trails</th>\n",
       "      <th>Lifts Open</th>\n",
       "      <th>Base Depth Notes</th>\n",
       "      <th>% Trails Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Arapahoe Basin Ski Area</td>\n",
       "      <td>2\"</td>\n",
       "      <td>52\"</td>\n",
       "      <td>133</td>\n",
       "      <td>145</td>\n",
       "      <td>8/9</td>\n",
       "      <td>Variable Conditions</td>\n",
       "      <td>91.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>Aspen Snowmass</td>\n",
       "      <td>9\"</td>\n",
       "      <td>53\"</td>\n",
       "      <td>349</td>\n",
       "      <td>366</td>\n",
       "      <td>34/41</td>\n",
       "      <td>Powder</td>\n",
       "      <td>95.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                   Resort Snow (24hr) Base Depth  Trails Open  \\\n",
       "0  2025-02-09  Arapahoe Basin Ski Area          2\"        52\"          133   \n",
       "1  2025-02-09           Aspen Snowmass          9\"        53\"          349   \n",
       "\n",
       "   Total Trails Lifts Open     Base Depth Notes  % Trails Open  \n",
       "0           145        8/9  Variable Conditions          91.72  \n",
       "1           366      34/41               Powder          95.36  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING THE TWO INTO ONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe are in the position now where we have two CSV files. THere must be a way to now take these and extract the best numbers form this into a new dataframe.\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We are in the position now where we have two CSV files. THere must be a way to now take these and extract the best numbers form this into a new dataframe.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset saved as 'C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\Merged_SnowReportScraper.csv' with backup safeguard.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Final Step --> Merge and Save as one final CSV\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "df_onthesnow = pd.read_csv(r\"snow_report_data_onthesnow.csv\")\n",
    "df_coloradoski = pd.read_csv(r\"snow_report_data_soloradoski.csv\")\n",
    "\n",
    "# Establish Mapping\n",
    "resort_name_mapping = {\n",
    "    \"Arapahoe Basin Ski Area\": \"Arapahoe Basin\",\n",
    "    \"Eldora Mountain Resort\": \"Eldora\",\n",
    "    \"Aspen Snowmass\": \"Snowmass\",\n",
    "    \"Monarch Mountain\": \"Monarch\",\n",
    "    \"Purgatory\": \"Purgatory Resort\",\n",
    "    \"Silverton Mountain\": \"Silverton\"\n",
    "}\n",
    "\n",
    "# Rename the Resorts so they can all be joined\n",
    "df_onthesnow['Resort'] = df_onthesnow['Resort'].replace(resort_name_mapping)\n",
    "\n",
    "# Merge the two datasets\n",
    "new_data = df_onthesnow.merge(right=df_coloradoski, how = \"left\", on = ['Resort', 'Date'])\n",
    "\n",
    "# Define the final file path\n",
    "final_csv_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\PersonalProjects\\SkiWebScraping\\Merged_SnowReportScraper.csv\"\n",
    "\n",
    "# Safety Check: Backup if the file exists\n",
    "if os.path.exists(final_csv_path):\n",
    "    # Create a backup with a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_path = f\"backup_snow_report_data_{timestamp}.csv\"\n",
    "    shutil.copy(final_csv_path, backup_path)\n",
    "    print(f\"Backup created at: {backup_path}\")\n",
    "\n",
    "    # Load the existing data\n",
    "    existing_data = pd.read_csv(final_csv_path)\n",
    "    \n",
    "    # Append new data without creating duplicates\n",
    "    combined_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['Resort', 'Date'], keep='last')\n",
    "else:\n",
    "    # No existing data, just use the new data\n",
    "    combined_data = new_data\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_data.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Final merged dataset saved as '{final_csv_path}' with backup safeguard.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
