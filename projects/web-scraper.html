<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Web Scraper Application - Daniel Volin">
    <title>Web Scraper Application | Daniel Volin</title>
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5966R6YDXX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-5966R6YDXX');
    </script>

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Plus+Jakarta+Sans:wght@600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">

    <link rel="stylesheet" href="../assets/css/style.css">
</head>
<body>

    <!-- Navigation -->
    <nav class="navbar navbar--interior">
        <div class="container">
            <a href="../index.html" class="navbar__brand">Danny Does Data</a>
            <div class="navbar__links">
                <a href="../index.html#about">About</a>
                <a href="../index.html#projects">Projects</a>
                <a href="../pages/today.html">Today's Training</a>
                <a href="../index.html#contact">Contact</a>
            </div>
            <button class="hamburger" aria-label="Toggle navigation" aria-expanded="false">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <!-- Project Hero -->
    <section class="hero hero--project" style="background: linear-gradient(135deg, #22C55E 0%, #059669 100%);">
        <div class="hero__content">
            <h1 class="hero__title">Web Scraper Application</h1>
            <p class="hero__subtitle">Automated ski resort snow report data collection</p>
            <div class="hero__buttons" style="margin-top: 1.5rem;">
                <span class="tag tag--white">Python</span>
                <span class="tag tag--white">Selenium</span>
                <span class="tag tag--white">BeautifulSoup</span>
                <span class="tag tag--white">Pandas</span>
                <span class="tag tag--white">Automation</span>
            </div>
        </div>
    </section>

    <!-- Project Content -->
    <div class="project-content">
        <a href="../index.html#projects" class="back-link"><i class="fas fa-arrow-left"></i> Back to Projects</a>

        <h2>Project Overview</h2>
        <p>
            This project provides automated scripts to collect real-time snow report data from two different websites:
            <strong>Colorado Ski</strong> and <strong>OnTheSnow</strong>. By leveraging Python's Selenium library and BeautifulSoup,
            the scraper captures details such as snowfall, base depth, open runs, and lifts open. The collected data are consolidated
            into CSV files for further analysis or integration into dashboards.
        </p>

        <h2>Objectives</h2>
        <ul>
            <li>Automate retrieval of up-to-date ski resort snow statistics.</li>
            <li>Combine data from multiple sources into a single dataset.</li>
            <li>Support historical trend analysis for snowfall and ski resort conditions.</li>
        </ul>

        <h2>Key Results</h2>
        <ul>
            <li>Scripts successfully fetch snow stats and resort availability data from two separate sites.</li>
            <li>Data is appended daily into CSV files, building a historical dataset.</li>
            <li>Resilient to dynamic page loads using Selenium's <code>WebDriverWait</code> for accurate scraping.</li>
            <li>Includes custom logging to help track runs, errors, and data integrity.</li>
        </ul>

        <h2>Methodology</h2>
        <ul>
            <li><strong>Selenium + ChromeDriver:</strong> Automates browser actions and waits for dynamic elements (e.g., 24-hour snowfall).</li>
            <li><strong>BeautifulSoup:</strong> Parses the HTML after page load to extract text and numerical values.</li>
            <li><strong>Data Aggregation:</strong> Each run appends new data to an existing CSV to build a time-series dataset.</li>
            <li><strong>Error Handling & Logging:</strong> Detailed logs for error detection and script debugging.</li>
        </ul>

        <h2>Skills Demonstrated</h2>
        <ul>
            <li><strong>Web Scraping:</strong> Handling dynamic sites using Selenium waits and parsing with BeautifulSoup.</li>
            <li><strong>Data Wrangling:</strong> Combining multiple data sources into CSV, cleaning irregular strings, and normalizing data fields.</li>
            <li><strong>Python Scripting:</strong> Managing environment paths, operating system checks, and structured logging.</li>
            <li><strong>Version Control:</strong> Storing and updating code via Git and GitHub.</li>
        </ul>

        <h2>Technologies Used</h2>
        <div style="display: flex; flex-wrap: wrap; gap: 0.5rem; margin-top: 0.5rem;">
            <span class="tag tag--python">Python</span>
            <span class="tag tag--selenium">Selenium</span>
            <span class="tag tag--beautifulsoup">BeautifulSoup</span>
            <span class="tag tag--pandas">Pandas</span>
            <span class="tag tag--webscraping">Web Scraping</span>
            <span class="tag tag--automation">Automation</span>
        </div>

        <h2>Scripts</h2>
        <p>
            Below is an abbreviated version of the two scripts used to scrape data from
            <strong>coloradoski.com</strong> and <strong>onthesnow.com</strong>.
        </p>

        <h3>Colorado Ski Scraper</h3>
<pre><code>import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

driver_path = "C:/path/to/chromedriver.exe"
csv_file_path = "C:/path/to/snow_report_data_coloradoski.csv"

# Set up and launch Chrome
service = Service(driver_path)
driver = webdriver.Chrome(service=service)
driver.get("https://www.coloradoski.com/snow-report/")

# Wait until mid-mountain depth loads
WebDriverWait(driver, 30).until(
    lambda d: any(
        mid.text.strip() != '0"'
        for mid in d.find_elements(By.CLASS_NAME, "answer.mid-mtn")
    )
)

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
driver.quit()

snow_data = []
resorts = soup.find_all("div", class_="inner")

for resort in resorts:
    name_tag = resort.find("h3", class_="h5 text-left")
    if not name_tag:
        continue

    name = name_tag.text.strip()
    snow_24hr = resort.find("span", class_="answer twentyfour").text.strip() \
                 if resort.find("span", class_="answer twentyfour") else "N/A"
    # ... repeat for other fields ...

    snow_data.append({
        "Date": datetime.now().strftime("%Y-%m-%d"),
        "Resort": name,
        "Snow (24hr)": snow_24hr,
        # ...
    })

new_data_df = pd.DataFrame(snow_data)

# Append or create CSV
if os.path.exists(csv_file_path):
    existing_data_df = pd.read_csv(csv_file_path)
    combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)
else:
    combined_df = new_data_df

combined_df.to_csv(csv_file_path, index=False)
print(new_data_df)</code></pre>

        <h3>OnTheSnow Scraper</h3>
<pre><code>import os
import re
import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

driver_path = r"C:\path\to\chromedriver.exe"
csv_file_path = r"C:\path\to\snow_report_data_onthesnow.csv"
log_file_path = r"C:\path\to\scraper_log.txt"

logging.basicConfig(filename=log_file_path, level=logging.INFO)

service = Service(driver_path)
driver = webdriver.Chrome(service=service)
driver.get("https://www.onthesnow.com/colorado/skireport")

WebDriverWait(driver, 40).until(
    EC.presence_of_element_located((By.CLASS_NAME, "styles_row__HA9Yq"))
)

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
driver.quit()

snow_data = []
resorts = soup.find_all("tr", class_="styles_row__HA9Yq")

for resort in resorts:
    data_tags = resort.find_all("span", class_="h4 styles_h4__x3zzi")
    # ... parse resort name, snowfall, base depth, open trails, etc. ...

new_data_df = pd.DataFrame(snow_data)

# Example of splitting base depth, computing % trails open, etc.
# ...
new_data_df.to_csv(csv_file_path, index=False)
logging.info("Data saved.")
print(new_data_df)</code></pre>

        <h2>GitHub Repository</h2>
        <p>
            <a href="https://github.com/davo1176/davo1176.github.io/tree/main/Projects/Snow%20Report%20Web%20Scraper/SkiWebScraping" target="_blank" class="btn btn--github">
                <i class="fab fa-github"></i> View on GitHub
            </a>
        </p>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer__content">
                <div class="footer__brand">Danny Does Data</div>
                <div class="footer__links">
                    <a href="../index.html#about">About</a>
                    <a href="../index.html#projects">Projects</a>
                    <a href="../index.html#contact">Contact</a>
                </div>
            </div>
            <p class="footer__copy">&copy; 2025 Daniel Volin</p>
        </div>
    </footer>

    <script src="../assets/js/main.js"></script>
</body>
</html>
